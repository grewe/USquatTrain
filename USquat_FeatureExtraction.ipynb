{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "USquat-FeatureExtraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXM78u12VWi8"
      },
      "source": [
        "# Check and import lirabries/packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMx7mVxzU82O"
      },
      "source": [
        "**1. Checking packpage/libraries version**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br_eRaWfVKzP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U88ueLShzEFS",
        "outputId": "46af12b7-398a-48ab-fe14-f42d6d927c60"
      },
      "source": [
        "import sys \n",
        "import tensorflow\n",
        "\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Tensorflow version: {tensorflow.__version__}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version: 3.6.9 (default, Oct  8 2020, 12:12:24) \n",
            "[GCC 8.4.0]\n",
            "Tensorflow version: 2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjUg4jwr0Fbs"
      },
      "source": [
        "**2. Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqDcUniCz7Gy"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "from sklearn.preprocessing import LabelBinarizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygz8jvFIVefn"
      },
      "source": [
        "# Get data from Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj7_gqKS0crP"
      },
      "source": [
        "**3. Mount Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEy7EaQq0acO",
        "outputId": "f64b3e14-6681-4d38-c0e7-b8f17bda29cd"
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('drive',force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_4tKKV-08A1"
      },
      "source": [
        "# checking if Drive is mounted and accessible\n",
        "# If shared folder does not appear by using the mount command, then reset Colab runtime\n",
        "# and mount using Web GUI (or vice versa)\n",
        "%cd /content/drive/MyDrive/Dataset/\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6dSseVl2AiM"
      },
      "source": [
        "**4. Setup PATHS & VARIABLES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhCoS-Mk2Hrz",
        "outputId": "e3862cfd-d79d-41db-d850-dd44af7c99dc"
      },
      "source": [
        "# Define Path to the Dataset folder\n",
        "BASE_PATH = '/content/drive/MyDrive/Dataset/MP4_OUTPUT'\n",
        "VIDEOS_PATH = os.path.join(BASE_PATH,'**','*.mp4')\n",
        "print(VIDEOS_PATH)\n",
        "\n",
        "# Define LSTM sequence length and batch_size\n",
        "SEQUENCE_LENGTH = 40\n",
        "BATCH_SIZE = 16"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Dataset/MP4_OUTPUT/**/*.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yweYB1B28pt"
      },
      "source": [
        "**5. Sample Video**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSYw7lu_3Cfm"
      },
      "source": [
        "We will not process every frame, but taking Kth sample where = num_frame_in_videos / SEQUENCE_LENGTH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qicZT4b2zGA"
      },
      "source": [
        "# taking Kth sameple function\n",
        "def frame_generator():\n",
        "    video_paths = tf.io.gfile.glob(VIDEOS_PATH)\n",
        "    np.random.shuffle(video_paths)\n",
        "    for video_path in video_paths:\n",
        "        frames = []\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        sample_every_frame = max(1, num_frames // SEQUENCE_LENGTH)\n",
        "        current_frame = 0\n",
        "        \n",
        "        label = os.path.basename(os.path.dirname(video_path))\n",
        "        \n",
        "        max_images = SEQUENCE_LENGTH\n",
        "        while True:\n",
        "            success, frame = cap.read()\n",
        "            if not success:\n",
        "                break\n",
        "            \n",
        "            # OpenCV reads in videos in BGR format so we need to rearrange the channels\n",
        "            # to be in RGB format, resize the image, and preprocess it for the CNN\n",
        "            if current_frame % sample_every_frame == 0:\n",
        "                frame = frame[:, :, ::-1]\n",
        "                img = tf.image.resize(frame, (224,224))\n",
        "                img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
        "                max_images -= 1\n",
        "                yield img, video_path\n",
        "                \n",
        "            if max_images == 0:\n",
        "                break\n",
        "            current_frame += 1"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ge7PedS5aQw"
      },
      "source": [
        "# Load Dataset\n",
        "dataset = tf.data.Dataset.from_generator(frame_generator,\n",
        "                                         output_types=(tf.float32,tf.string),\n",
        "                                         output_shapes=((224,224,3),()))\n",
        "\n",
        "# set batch_size \n",
        "dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpBUjSac558j",
        "outputId": "58087bff-d9db-44fe-9e81-e66148ecf2f0"
      },
      "source": [
        "# Print shape and type of dataset\n",
        "print(dataset)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.string)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eHbpLQFVmgW"
      },
      "source": [
        "# Build Feature Extraction Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hj2_LdC6aZd"
      },
      "source": [
        "**6. Build Feature Extraction Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39ARk3cJ6HKC"
      },
      "source": [
        "mobilenet_v2 = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(224,224,3),\n",
        "                                                              include_top=False,\n",
        "                                                              weights='imagenet')\n",
        "x = mobilenet_v2.output\n",
        "\n",
        "# Average Pooling - transforming the feature map from 8*8*2048 to 1x2048\n",
        "pooling_output = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "feature_extraction_model = tf.keras.Model(mobilenet_v2.input,pooling_output)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzKsZ2D37IT6"
      },
      "source": [
        "# ONLY RUN ONCE\n",
        "# Extract Feature - Generate .npy file for each video\n",
        "\n",
        "current_path = None\n",
        "all_features = []\n",
        "\n",
        "# cycle through each img and extracts its features\n",
        "# tqdm is a progress bar which updates each time the feature_extraction_model is called \n",
        "\n",
        "for img, batch_path in tqdm.tqdm(dataset):\n",
        "  batch_features = feature_extraction_model(img)\n",
        "  # Reshape tensor\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1))\n",
        "  \n",
        "  for features, path in zip(batch_features.numpy(), batch_path.numpy()):\n",
        "    if path != current_path and current_path is not None:\n",
        "      output_path = current_path.decode().replace('.mp4','.npy')\n",
        "      np.save(output_path,all_features)\n",
        "      all_features = []\n",
        "\n",
        "    current_path = path\n",
        "    all_features.append(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUvs2nLt_dDM"
      },
      "source": [
        "**7. Create Labels of our classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIgRYxwK8yf4",
        "outputId": "0c325b22-7d65-47ec-ac39-57acbdde30e0"
      },
      "source": [
        "# load LABELS \n",
        "LABELS = ['good','heels_off','bent_over','knees_forward','knees_in','shallow']\n",
        "# Encode to 0- not belong to class Or 1- belong to class using LabelBinarizer()\n",
        "encoder = LabelBinarizer()\n",
        "encoder.fit(LABELS )"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhCjaE6kCHRk",
        "outputId": "8597250e-5bf0-4d24-a41e-ce2945066821"
      },
      "source": [
        "# Checking output of LabelBinarizer()\n",
        "print(encoder.classes_)\n",
        "print(encoder.transform(LABELS))\n",
        "print(encoder.inverse_transform(t))"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bent_over' 'good' 'heels_off' 'knees_forward' 'knees_in' 'shallow']\n",
            "[[0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0]\n",
            " [1 0 0 0 0 0]\n",
            " [0 0 0 1 0 0]\n",
            " [0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1]]\n",
            "['good' 'heels_off' 'bent_over' 'knees_forward' 'knees_in' 'shallow']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0JO9js8DXh-"
      },
      "source": [
        "**8. LSTM Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRmMiVRMDf1P"
      },
      "source": [
        "Define LSTM Model with following layers:\n",
        "\n",
        "\n",
        "*   Layer 1 = Masking Layer ( see keras [Doc](https://keras.io/api/layers/core_layers/masking/) )\n",
        "*   Layer 2 = Defind what **1** cell of LSTM looks like [LSTM layer](https://keras.io/api/layers/recurrent_layers/lstm/). The total number of cells is defined as SEQUENCE_LENGTH above\n",
        "*   Layer 3 = FNC (fully-connected layer) relu activation ( see [Dense layer](https://keras.io/api/layers/core_layers/dense/) )\n",
        "*   Layer 4 = Drouput layer\n",
        "*   Layer 5 = final decision FNC layer with softmax activation -- output has length of classes \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU8IlI1tCWWP",
        "outputId": "92482946-0f8a-4b8c-d467-52c6c461ab57"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Masking(mask_value=0.),\n",
        "                             tf.keras.layers.LSTM(512,dropout=0.5,recurrent_dropout=0.5), # 512 units (hidden-layer)\n",
        "                             tf.keras.layers.Dense(256,activation='relu'),\n",
        "                             tf.keras.layers.Dropout(0.5),\n",
        "                             tf.keras.layers.Dense(6,activation='softmax')\n",
        "])\n",
        "print(len(LABELS))"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS6g_kndHCKz"
      },
      "source": [
        "# Setup Loss function and metrics (see more at https://keras.io/api/metrics/)\n",
        "model.compile(loss='categorical_crossentropy', # since we want to classify different type of squats\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy','top_k_categorical_accuracy']) # focus on accuracy"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrWAKINPVs4d"
      },
      "source": [
        "# Split Training/Test Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbePgF19HevP"
      },
      "source": [
        "**9. Splitting Training/Test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVWuSJmFHZ2z",
        "outputId": "b582c46a-6c8a-4898-ef2b-3dd58cea35c6"
      },
      "source": [
        "data_path=[]\n",
        "for root,dir,filename in os.walk(BASE_PATH,topdown=True):\n",
        "  for video_path in filename:\n",
        "    if video_path.endswith('.mp4'):\n",
        "      data_path.append(os.path.join(root,video_path))\n",
        "\n",
        "# Shuffle video_paths and split to training-test(80-20)\n",
        "import random     \n",
        "print(len(data_path))\n",
        "random.shuffle(data_path)\n",
        "train_list = data_path[int((len(data_path)+1)*.20):] # get 80% of data to training\n",
        "test_list = data_path[:int((len(data_path)+1)*.20)] # the rest go to testing\n",
        "\n",
        "# number of train\n",
        "print(f'Training: {len(train_list)}')\n",
        "# number of test \n",
        "print(f'Test/Valid: {len(test_list)}')\n",
        "\n"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "289\n",
            "Training: 231\n",
            "Test/Valid: 58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcPk7fL6JUgb"
      },
      "source": [
        "# define make_generator() that returns a generator which will randomly shuffle the video list\n",
        "# then building out the list as the .npy feature files\n",
        "\n",
        "def make_generator(file_list):\n",
        "  def generator():\n",
        "    np.random.shuffle(file_list)\n",
        "    for path in file_list:\n",
        "      full_path = path.replace('.mp4','.npy')\n",
        "      label = os.path.basename(os.path.dirname(path))\n",
        "      features = np.load(full_path)\n",
        "\n",
        "      padded_sequence = np.zeros((SEQUENCE_LENGTH,1280)) # MobileNet feature extractor \n",
        "      padded_sequence[0:len(features)] = np.array(features)\n",
        "\n",
        "      transform_label = encoder.transform([label])\n",
        "      yield padded_sequence, transform_label[0]\n",
        "  return generator"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NVnLf0FRE1B"
      },
      "source": [
        "# setting Training/Test(Validation) Data\n",
        "# Since we would use a new/unseen dataset for testing, thus the testing set here is used for validation\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(make_generator(train_list),\n",
        "                                               output_types=(tf.float32,tf.int16),\n",
        "                                               output_shapes=((SEQUENCE_LENGTH,1280),(len(LABELS))))\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(make_generator(test_list),\n",
        "                                              output_types=(tf.float32,tf.int16),\n",
        "                                              output_shapes=((SEQUENCE_LENGTH,1280),(len(LABELS))))\n",
        "\n",
        "valid_dataset = valid_dataset.batch(BATCH_SIZE,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Print Train and Valid dataset\n",
        "print(train_dataset)\n",
        "print(valid_dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npzR1ypmU1Fe"
      },
      "source": [
        "# Train LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3yypD4DTn6O"
      },
      "source": [
        "**10. Train LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oldVX3t7ULG-"
      },
      "source": [
        "# Create Log_dir for TensorBoard Visualization\n",
        "ROOT_PATH = '/content/drive/MyDrive/Dataset'\n",
        "LOG_DIR = os.path.join(ROOT_PATH,'training_log')\n",
        "\n",
        "# Create new dir if not exists\n",
        "if not os.path.isdir(LOG_DIR): \n",
        "  !mkdir training_log\n"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIC3ZNa2SkP7"
      },
      "source": [
        "# Callback function that will store information (checkpoints,etc) used for TensorBoard\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR,update_freq=1000)\n",
        "model.fit(train_dataset,epochs=10,callbacks=[tensorboard_callback],verbose=2,validation_data=valid_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpn664SZVgRo"
      },
      "source": [
        "# Save model\n",
        "model.file = os.path.join(BASE_PATH,'feature_extraction.h5')\n",
        "model.save(model.file)"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7v5U6HdV28c"
      },
      "source": [
        "# Tensorboard and Evaluation on test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkqCiXVj4hkh"
      },
      "source": [
        "**11. Tensorboard Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4X3hGru4GNQ"
      },
      "source": [
        "# Load the tensorboard notebook\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {LOG_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXu6VqE66I9F"
      },
      "source": [
        "**12. Run Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc77H-eC48X1",
        "outputId": "d25ef1cd-065f-4926-8d43-62ae2c8a31e1"
      },
      "source": [
        "print('----------------')\n",
        "print('Evaluate on test Data')\n",
        "\n",
        "# define test_set path \n",
        "TEST_PATH = ''\n",
        "test_result = model.evaluate(test_dataset, verbose=1)\n",
        "print(f'Test Loss, Test Accuracy: {test_result}')"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------\n",
            "Evaluate on test Data\n",
            "3/3 [==============================] - 1s 267ms/step - loss: 1.4319 - accuracy: 0.5208 - top_k_categorical_accuracy: 1.0000\n",
            "Test Loss, Test Accuracy: [1.4319251775741577, 0.5208333134651184, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSFaN3-D6qqC"
      },
      "source": [
        "**13. Run Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1kMG-aV6hMf"
      },
      "source": [
        "prediction = model.predict(valid_dataset,verbose=1)\n",
        "\n",
        "print(prediction.shape)\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSpqwQac7Bsd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}